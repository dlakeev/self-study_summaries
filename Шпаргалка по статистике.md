# Оглавление <a id="0"></a>

1. [Введение](#1)
    1. [Генеральная совокупность и выборка](#1.1)
    2. [Описательная статистика](#1.2)
    3. [Меры центральной тенденции](#1.3)
    4. [Меры изменчивости](#1.4)
    5. [Квартили распределения и график box-plot](#1.5)
    6. [Нормальное распределение](#1.6)
    7. [Z-преобразование](#1.7)
    8. [Центральная предельная теорема](#1.8)
    9. [Доверительные интервалы](#1.9)
    10. [Идея статистического вывода, p-value](#1.10)
2. [Сравнение средних](#2)
    1. [Параметрические и непараметрические методы](#2.1)
    2. [T-распределение](#2.2)
    3. [Сравнение двух средних; Парный t-test; t-критерий Стьюдента](#2.3)
    4. [Проверка распределения на нормальность](#2.4)
    5. [Однофакторый дисперсионный анализ](#2.5)
    6. [Множественные сравнения](#2.6)
    7. [Многофакторный ANOVA](#2.7)
    8. [A/B тесты и статистика](#2.8)
3. [Корреляция и регрессия](#3)
    1. [Понятие корреляции](#3.1)
    2. [Регрессия с одной независимой переменной](#3.2)
    3. [Гипотеза о значимости взаимосвязи и коэффициент детерминации](#3.3)
    4. [Применение регрессионного анализа и интерпретация результатов](#3.4)
    5. [Регрессионный анализ с несколькими независимыми переменными](#3.5)
4. [Анализ номинативных данных](#4)
    1. [Расстояние Пирсона](#4.1)
    2. [Распределение хи-квадрат](#4.2)
    3. [Анализ таблиц сопряженности](#4.3)
    4. [Точный критерий Фишера](#4.4)
5. [Логистическая регрессия и непараметрические методы](#5)
    1. [Логистическая регрессия](#5.1)
    2. [Когда нужно использовать непараметрические методы](#5.2)
    3. [U-критерий Манна — Уитни](#5.3)
    4. [Критерий Краскела-Уоллиса](#5.4)
    5. [Дорожная карта статистики](#5.5)

___
# 1. Введение <a id="1"></a>
## 1.1 Генеральная совокупность и выборка <a id="1.1"></a>
**_Генеральная совокупность (далее ГС)_** - совокупность всех объектов, относительно которых предполагается делать выводы при изучении некоторой задачи.

**_Репрезентативная выборка_** - это выборка, обладающая свойствами ГС.

Выборки могут быть:

* random
* stratified
* cluster

| Групповая выборка                                                                                                                | Стратифицированная выборка                                                                              |
|----------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------|
| Выборка формируется только из нескольких субпопуляций (кластеров)                                                                 | Выборка формируется из всех субпопуляций (страт)                                                        |
| В пределах кластера элементы должны быть разнородны, тогда как поддерживается однородность или схожесть между разными кластерами | В пределах страты элементы должны быть однородны, а между стратами должна быть разнородность (различия) |
| Схема выборки нужна только для кластеров, попавших в выборку                                                                     | Должна быть сформирована полная схема выборки для всех стратифицированных субпопуляций                  |
| Повышает эффективность выборки, уменьшая стоимость                                                                               | Повышает точность                                                                                       |

___
## 1.2 Описательная статистика <a id="1.2"></a>
**_Эмпирические данные_** - данные, полученные опытным путем.

**_Описательная статистика_** - обработка данных, полученных эмпирическим путем и их систематизации, наглядное представление в виде графиков и таблиц.

**_Распределение вероятностей_** - это закон, описывающий область значений случайной величины и вероятность её появления (частоту) в данной области. То есть насколько часто $X$ появляется в данном диапазоне значений.

**_Гистограмма частот_** - ступенчатая функция показывающая насколько часто вероятно появление величины в указанном диапазоне значений.
___
## 1.3 Меры центральной тенденции <a id="1.3"></a>
* **_Мода_** - это значение признака, которое встречается максимально часто. В выборке может быть несколько или одна мода.
* **_Медиана_** - это значение признака, которое делит упорядочное множество попалам. Если множество содержит чётное количество элементов, то берётся среднее из двух серединных элементов упорядочного множества.
* **_Среднее_** - сумма всех значений измеренного признака делится на количество измеренных значений. Свойства:
$$M_{x+c}=M_{x}+c$$
$$M_{x*c}=M_{x}*c$$
$$\sum_{i=1}^{n}(x_{i}-M_{x})=0$$

<p align="center">
  <img src="https://ucarecdn.com/71c5315f-1020-4606-a402-7216a116bae9/">
</p>

___
## 1.4 Меры изменчивости <a id="1.4"></a>
* **_Размах_** - это разность между максимальным и минимальным значениям выборки. Изменение крайних значений очень влияет на результат.
$$R = X_{max}-x_{min}$$
* **_Дисперсия_** - это средний квадрат отклонений индивидуальных значений признака от их средней величины (насколько в среднем значения отклоняются от среднего).
$$D = \frac{\sum_{i=1}^{n}(x_{i}-M_{x})^2}{n}$$
$$D = \frac{\sum_{i=1}^{n}(x_{i}-M_{x})^2}{n-1}$$
Первая формула - для ГС. n - 1 в выборке использутеся для корректировки (связано со степенями свободы).
  _Свойства_:
$$D_{x+c} = D_{x}$$
$$D_{x*c} = D_{x}*c^2$$
* **_Среднеквадратичное отклонение_** - это корень дисперсии. Лучше отражает изменчивость, так как показатель дисперсии завышен из-за квадрата в числителе.
  
  $σ$ - для ГС. $sd$ - для выборки (стандартное отклонение).
  
  _Свойства_:
$$sd_{x+c}=sd_{x}$$
$$sd_{x*c}=sd_{x}*c$$

<p align="center">
  <img src="/img/Меры_изменчивости.png">
</p>

**_Коэффициент вариации_**, также известный как относительное стандартное отклонение, — это стандартная мера дисперсии распределения вероятностей или частотного распределения:

$$CV=\frac{σ}{M}$$

___
## 1.5 Квартили распределения и график box-plot <a id="1.5"></a>
**_Квартили_** - $3$ точки, которые делят упорядоченное множество на $4$ равные части.

**_Box plot_** (ящик с усиками) - удобный вид диаграммы для статистических показателей.

<p align="center">
  <img src="https://ucarecdn.com/c476e686-fb7b-4e8a-b9d3-54d3b461bb44/">
</p>

$Q$ и $IQR$ используют, чтобы оценить наличие выбросов. Алгоритм расчета - посчитать $Q$-ли, разницу между ними, вычислить теоретический максимум и минимум, сравнить с имеющимися и выяснить, есть ли выбросы и сколько их.

<p align="center">
  <img src="https://ucarecdn.com/890a7dfd-5a9b-4a08-b0fc-7e9b15488f78/">
</p>

___
## 1.6 Нормальное распределение <a id="1.6"></a>
* Унимодально
* Симметрично
* Отклонения наблюдений от среднего подчиняются определенному вероятностному закону; равновероятны.

<p align="center">
  <img src="https://raw.githubusercontent.com/KlukvaMors/basic_stat/a3db1e2526a8092d70d988531e8b10ad39ee8f32//img/%D0%B3%D0%B0%D0%BB%D1%8C%D1%82%D0%BE%D0%BD.gif">
</p>

### Другие распределения

<p align="center">
  <img src="https://miro.medium.com/max/962/1*DmPUIjvecL7KllOamoFSDw.png">
</p>

___
## 1.7 Z-преобразование <a id="1.7"></a>
Преобразование полученных данных в стандартную z-шкалу **_со средним значением=0_** и **_дисперсией=1_**.

Чтобы привести к такому виду из каждого наблюдения нужно отнять среднее значение и разделить на стандартное отклонение:
$$Z_{i}=\frac{x_{i} - \bar{X}}{sd}$$
Иногда необходимо расчитать z-значение только для отдельно взятого наблюдения, чтобы выяснить, насколько далеко оно отклоняется от среднего значения в единицах стандартного отклонения.

Такое преобразование никак не влияет на форму распределения.

Чтобы посмотреть, насколько стандартных отклонений отклонилось выборочное среднее от среднего генеральной совокупности используется формула: 
$$Z=\frac{\bar{X}-M}{se}$$

**_Правило сигм в нормальном распределении_**:
* $M_x±σ\approx68$%
* $M_x±2σ\approx95$%
* $M_x±3σ\approx100$%

<p align="center">
  <img src="https://raw.githubusercontent.com/KlukvaMors/basic_stat/a3db1e2526a8092d70d988531e8b10ad39ee8f32//img/3-sigma.svg">
</p>

[Distribution Calculator](https://gallery.shinyapps.io/dist_calc/)

[Таблица z-значений. Процент наблюдений, не превышающий указанное z-значение](https://web.archive.org/web/20180729102938/http://users.stat.ufl.edu/~athienit/Tables/Ztable.pdf)

**Задача №1**

|        Дано:        |               Решение:               |
|:-------------------:|:------------------------------------:|
|   $$\bar{X}=100$$   |           $sd=\sqrt{D}=5$            |
|       $$D=25$$      | $95$% $\approx\bar{X}+2sd={\color{Green}(90, 100)}$ |
|     **Найти:**      |                                      |
| Диапазон $95$% данных |    

**Задача №2**

|      Дано:      |                                          Решение:                                         |
|:---------------:|:-----------------------------------------------------------------------------------------:|
| $$\bar{X}=100$$ |                         $$z_{125}=\frac{x-\bar{X}}{sd}\approx1.7$$                        |
|    $$sd=15$$    | По таблице z-преобразования при $z=1.7$ - $95$% данных лежат слева от искомого значения. |
|    **Найти:**   |                            Значит $(IQ > 125)$% $= 100$%- $95$% = ${\color{Green}5}$%                            |
|   $(IQ > 125)$%  |                                                                                           |

**Задача №3**

|       Дано:      |                         Решение:                        |
|:----------------:|:-------------------------------------------------------:|
|  $$\bar{X}=100$$ |            $z_{70}=-2$ (Слева $2.28$% данных)           |
|     $$sd=15$$    |$z_{112}=0.8$ (Слева $78.81$% данных или справа $21.19$%)|
|    **Найти**:    |              $100- 2.28 -21.19\approx{\color{Green}77}$%               |
|$(70 < IQ < 112)$%|                                                         |

___
## 1.8 Центральная предельная теорема <a id="1.8"></a>
ЦПТ гласит, что множество средних выборок из генеральной совокупности (_ГС необязательно должно иметь нормальное распределение_) будут иметь _нормальное распределение_.

Причем средняя этого распределения будет _близко к средней ГС_, а sd этого распределения будет называться **стандартной ошибкой среднего (se)**.

$$se=\frac{σ}{\sqrt{N}}$$

Если $N$ достаточно большое $(> 30)$ и выборка является репрезентативной, то вместо σ можно взять $sd$.

* Стандартная ошибка среднего - среднеквадратическое отклонение распределения выборочных средних.
* Стандартная ошибка всегда меньше, чем стандартное отклонение при $n > 1$.
* $se$ тем меньше, чем больше $N$ и меньше вариативность исследуемого признака.
* Чем меньше $se$, тем реже выборочные средние будут сильно отклоняться от среднего в ГС.
___
## 1.9 Доверительные интервалы <a id="1.9"></a>
Если мы имеем некоторую выборку и ГС, то мы не можем точно знать среднюю ГС, зная только среднее выборки. Однако мы можем сказать с некоторым процентом уверенности, в каком интервале лежит средняя ГС.

Средняя средних выборок стремится к средней ГС, а se среднего описывает $sd$ распределения средних выборок.

Если взять случайную выборку $X$ и найти $\bar{X}$, вычислить se, то можно найти доверительный интервал $(DI)$, который описывает среднюю ГС в некотором интервале с $n$% доверия.

* $DI(95)=\left [ \bar{X}\pm1.96*se \right ]$
* $DI(99)=\left [ \bar{X}\pm2.58*se \right ]$

Если рассчитать $95$% $DI$ для среднего значения, то:

* Мы можем быть на $95$% уверены, что $M$ в ГС принадлежит рассчитанному $DI$.
* Если многократно повторять эксперимент, для каждой выборки рассчитывать свой $DI$, то в $95$% случаев истинное среднее будет находиться внутри $DI$.
___
## 1.10 Идея статистического вывода, $p$-value <a id="1.10"></a>
Интерпретация $p$-уровня значимости:
* $p$-уровень значимости ничего не позволяет сказать о том, с какой вероятностью верна нулевая гипотеза $H_0$
* Если $p=0.05$, то это означает, что, если верна $H_0$, вероятность получить такие или еще более выраженные различия, равна $0.05$.
* $p$ не сообщает о силе эффекта, величине различий.
* Если $p>0.05$, то недостаточно оснований, чтобы отклонить $H_0$.
* Чтобы считать значения статистически достоверными, используется двусторонний p-уровень значимости, учитывая отклонения в обе стороны.
* Если отклонять $H_0$ рискованно, то нужно ставить маленький уровень значимости $\lambda$ - например, $0.01$.
* $DI$ - альтернатива $p$-value. Если значение $H_0$ не будет принадлежать $DI(95)$, то достаточно оснований отклонить $H_0$. 

**Статистические ошибки:**
Обычно нулевой гепотизой является то что мы хотим опровергнуть - это так называемый _Reject-Support_. Мы как исследователи хотим чтобы нуль-гипотеза был опровергнута  и при этом мы боимся совершить ошибку второго типа (потому что мы упустим возможность что-то исследовать) - поэтому мы будем набирать данные и пробовать таки опровергнуть ее.

А вот общество не хочет чтобы мы совершили ошибку Первого Типа - потому что последующие работы будут основаны на не верных выводах и в будущем все буду разочарованы.

<p align="center">
  <img src="https://ucarecdn.com/7fbb0394-b60e-410d-85ff-2aa0fd2f7e60/">
</p>

**Задача №4**

|        Дано:       |                             Решение:                            |
|:------------------:|:---------------------------------------------------------------:|
|       $M=20$       |                 1) $se=\frac{sd}{\sqrt{N}}=0.5$                 |
|       $N=64$       |                   $z=\frac{\bar{X}-M}{se}=-3$                   |
|   $\bar{X}=18.5$   |             По свойствам нормального распределения:             |
|       $sd=4$       |                   $P(X<-3, X>3)=0.0027<0.05$                  |
|   $H_0:M_{нп}=20$  |     2) $DI(95)=\left [ \bar{X}\pm*se \right ] = \left [ 17.52;19.48 \right ]$   |
| $H_1:M_{нп}\neq20$ | Следовательно, $20$ не принадлежит $DI(95)$ - отклоняем $H_0$  |
|     **Найти**:     |                $100 -2.28 -21.19 \approx{\color{Green}77}$%           |
|         $p$        |                                                                 |
|     $DI(95)$      |                                                                 |
___
# 2. Сравнение средних <a id="2"></a>
## 2.1 Параметрические и непараметрические методы <a id="2.1"></a>
**_Параметрические методы_**:
* Основанием таких методов являются вполне вероятные предположения о характере распределения случайной величины.
* Чаще всего эти методы используются в анализе экспериментальных данных и предположении _нормальности распределения_ этих данных.
* Метод параметрического анализа имеет достоинство, заключающееся в том, что обладает высокой мощностью, т.е. способностью избегать ошибки второго рода или $β$-ошибки.
* Параметрические тесты требуют специальных метрических шкал для описания имеющихся данных.
* Параметрическая статистика имеет постоянное число параметров и делает больше предположений.
* При правильности дополнительных предположений, параметрические методы дают более точные оценки. При неправильности предположений параметрические методы могут исследователя ввести в заблуждение.
* Параметрические формулы, однако, просты, их можно быстро записать и так же быстро вычислить.

**_Непараметрические методы_**:
* Непараметрическими называются такие методы, при которых не происходит выдвижение каких-либо предположений о характере распределения исследуемых данных.
* Непараметрические методы широко используются для изучения популяций, которые принимают ранжированный порядок (например, обзоры фильмов, которые могут получать от одной до четырех звезд). Использование непараметрических методов может быть необходимым, когда данные имеют ранжирование, но не имеют ясной численной интерпретации, например, при оценке предпочтений.
* Результатами работы непараметрических методов являются порядковые данные.

<p align="center">
  <img src="https://fs.znanio.ru/d5af0e/86/f3/6601150f67e76adbf56600115fb80c7553.jpg">
</p>

___
## 2.2 T-распределение <a id="2.2"></a>
Если $N$ невелико и $σ$ неизвестно (почти всегда), используется **_распределение Стьюдента_** или $t$-distribution.

T-распределение - унимодально и симметрично, но наблюдения с большей вероятностью попадают за пределы $\pm2σ$ от $M$.

"Форма" распределения определяется **_числом степеней свободы_** $(df=n-1)$. С увеличением $df$ распределение стремится к _нормальному_. Если у нас есть $10$ наблюдений, и мы знаем среднее значение этих наблюдений, то достаточно знать среднее и только $9$ элементов, чтобы полностью знать, чему равен десятый элемент.

При расчете различных статистических показателей важно учитывать, как много элементов независимых было использовано, чтобы получить итоговое значение - это может приводить к абсоютно разным выводам.

Распределение отклонения выборочного среднего и среднего в генеральной совокупности, деленного на стандартную ошибку, будет описываться при помощи $t$-распределения: $t=\frac{\bar{X}-M}{\frac{sd}{\sqrt{n}}}$ в случаях, когда не известно $σ$ (далее при $n>30$).
___
## 2.3 Сравнение двух средних; Парный t-test; t-критерий Стьюдента <a id="2.3"></a>
Наиболее часто $t$-критерий применяется для проверки равенства средних значений в двух выборках. $H_0$ предполагает, что средние равны (отрицание этого - гипотеза сдвига).

Для применения данного критерия необходимо, чтобы исходные данные имели нормальное распределение.

$$t=\frac{\bar{X_1}-\bar{X_2}}{se}=\frac{\bar{X_1}-\bar{X_2}}{\sqrt{\frac{sd_{1}^2}{n_1}+\frac{sd_{2}^2}{n_2}}}$$

Применяя данный критерий, нужно, чтобы соблюдались требования:
* **_гомогенности_** (дисперсии внутри групп были приблезительно одинаковы - проверить можно с помощью _Критерия Фишера_ или _Тест Левена_)
* если число элементов $< 30$, то очень важно, чтобы данные внутри выборок были распеределены _нормально_. Если $> 30$, то t-test неплохо справляется с поставленной задачей, даже если распределение внутри выборок отличается от нормального.

**_Примеры применения t-критерий Стьюдента_**

1. Первая выборка — это пациенты, которых лечили препаратом А. Вторая выборка — пациенты, которых лечили препаратом Б. Значения в выборках — это некоторая характеристика эффективности лечения (уровень метаболита в крови, температура через три дня после начала лечения, срок выздоровления, число койко-дней, и т.д.) Требуется выяснить, имеется ли значимое различие эффективности препаратов А и Б, или различия являются чисто случайными и объясняются «естественной» дисперсией выбранной характеристики.

2. Первая выборка — это значения некоторой характеристики состояния пациентов, записанные до лечения. Вторая выборка — это значения той же характеристики состояния тех же пациентов, записанные после лечения. Объёмы обеих выборок обязаны совпадать; более того, порядок элементов (в данном случае пациентов) в выборках также обязан совпадать. Такие выборки называются связными. Требуется выяснить, имеется ли значимое отличие в состоянии пациентов до и после лечения, или различия чисто случайны.

3. Первая выборка — это поля, обработанные агротехническим методом А. Вторая выборка — поля, обработанные агротехническим методом Б. Значения в выборках — это урожайность. Требуется выяснить, является ли один из методов эффективнее другого, или различия урожайности обусловлены случайными факторами.

4. Первая выборка — это дни, когда в супермаркете проходила промо-акция типа А (красные ценники со скидкой). Вторая выборка — дни промо-акции типа Б (каждая пятая пачка бесплатно). Значения в выборках — это показатель эффективности промо-акции (объём продаж, либо выручка в рублях). Требуется выяснить, какой из типов промо-акции более эффективен.
___
## 2.4 Проверка распределения на нормальность <a id="2.4"></a>
Используется Q-Q Plot

<p align="center">
  <img src="https://ucarecdn.com/8b33fe84-ad8a-4928-87c0-bd0cd1b19492/">
</p>

На графике: ось $X$ - предсказанные значения, если распределение идеально нормальное; ось $Y$ - реальные значения признака.

* Если точки выше прямой, то значениия слишком высокие, чем должны были получить.
* Если точки ниже прямой, то значениия слишком маленькие.

Для проверки на нормальность используется **_Shapiro-Wilk test_**. В таком тесте $p$-уровень значимости $< 0.05$ сообщает плохую новость, ведь придется отклонить $H_0$ о том, что данные распределены нормально.

_Выбросы значительно_ влияют на $t$-критерий. Менее чувствительной к выбросам **_Mann-Whitney U-Test_**, непараметрический аналог $t$-критерия.
___
## 2.5 Однофакторый дисперсионный анализ <a id="2.5"></a>

Переменная, разделяющая на группы - **_независимая_**.

Переменная, по которой идет сравнение - **_зависимая_**.

$t$-критерий и его непараметрические аналоги (например, Mann-Whitney U-test) предназначены для сравнения _исключительно 2 совокупностей_. В случаях $2+$ применяется $F$-test.

* Теоретическое распределение $F$-значения не является нормальным.
* $F$ всегда положительно, поэтому вероятность отклонения рассчитывается только в правую сторону.

$$SS_{total}=SS_{between}+SS_{within}$$

, где **_Межгрупповой_**:
* $SS_{between}=\sum_{j=1}^{p}n_j(x_{j}-\bar{X})^2$
* $df_{between}=m-1$
* $MS_{between}=\frac{SS_{between}}{df_{between}}$ (**_Межгрупповой средний квадрат_**)

**_Внутригрупповой_**:
* $SS_{within}=\sum_{j=1}^{p}\sum_{i=1}^{n_j}(x_{ij}-\bar{X_j})^2$
* $df_{within}=N-m$, где $m$ - _количество групп_
* $MS_{within}=\frac{SS_{within}}{df_{within}}$ (**_Внутригрупповой средний квадрат_**)

$$F=\frac{\frac{SS_{between}}{df_{between}}}{\frac{SS_{within}}{df_{within}}}=\frac{MS_{between}}{MS_{within}}$$

_Пример записи_: удалось выявить **статистически значимую взаимосвязь** $(F(3, 56)=8.04,p<0.05)$.

**Задача №5**

|                                              Дано:                                              |                                         Решение:                                         |
|:-----------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------:|
| $\begin{pmatrix} 1group & 2group & 3group\\  3 & 5 & 7\\  1 & 3 & 6\\  2 & 4 & 5 \end{pmatrix}$ |               $\bar{\bar{X}}=\frac{3+1+2+5+3+4+7+6+5}{9}=4$ - общая средняя              |
|                                        $H_0:M_1=M_2=M_3$                                        |                       $SS_{between}=3(2-4)^2+3(4-4)^2+3(6-4)^2=24$                       |
|                         $H_1:$ Как минимум 2 группы значимо различаются                         |                                     $df_{between}=2$                                     |
|                                                                                                 | $SS_{within}=(3-2)^2+(1-2)^2+(2-2)^2+(5-4)^2+(3-4)^2+(4-4)^2+(7-6)^2+(6-6)^2+(5-6)^2=6$  |
|                                                                                                 | при $\bar{X_1}=2; \bar{X_2}=4; \bar{X_3}=6$                                              |
|                                                                                                 | $df_{within}=6$                                                                          |
|                                                                                                 | $F=\frac{\frac{24}{2}}{\frac{6}{6}}=12$                                                  |
|                                                                                                 | $P(X>12)<0.05\Rightarrow H_0$ отклоняется                                                |

___
## 2.6 Множественные сравнения <a id="2.6"></a>
**_Внимание_**: Если многократно увеличивать количество групп, участвующих в исследовании, и сравнивать их между собой, то вероятность получить хотя бы одно значимое различие, которое на самом деле может и не быть, значительно увеличивается!

При множественном парном сравнении _необходимо корректировать уровень значимости_ для сравнения с $p$-уровнем.

Количество парных сравнений для выборок: $\frac{n(n-1)}{2}$, где n - число выборок.

**_Поправка Бонферрони_**: $\frac{\lambda}{\frac{n(n-1)}{2}}$

Бонферрони - довольно консервативное решение; при $100$ сравнениях, гарантируя отсутствие хотя бы одного ложного, упускается до $88$% реальных открытий.

**_Критерий Тьюики_**: $q=\frac{\bar{X_B}-\bar{X_A}}{se}=\frac{\bar{X_B}-\bar{X_A}}{\sqrt{\frac{MS_{within}}{n}}}$
___
## 2.7 Многофакторный ANOVA <a id="2.7"></a>
В многофакторном анализе $SS_{total}=SS_{between}+SS_{within_A}+SS_{within_B}+SS_{within_A}*SS_{within_B}$

**_Взаимодействие факторов_**: некоторая переменная оказывает различное влияние на интересующий показатель в зависимости от уровня или градации другой независимой переменной.

_Требования к данным_:
* Нормальность распределения в каждой из групп;
* Гомогенность дисперсий.

При $N > 50$ Anova _устойчива к нарушению_ обоих требований.
___
## 2.8 A/B тесты и статистика <a id="2.8"></a>
* [Создания A/B тестов на языке Python.]('https://vkteam.medium.com/practitioners-guide-to-statistical-tests-ed2d580ef04f')
* [Как правильно выстроить работу b2c: на какие метрики необходимо смотреть и как их учитывать для улучшения работы.]('https://onlineuserengagement.github.io/')

Пользователи должны делиться на **_контрольную_** $(A)$ и **_тестовую_** $(B)$ группы в рамках _одинаковых условий_.

**_Проектирование эксперимента_** $A/B$-тестирования (условия формулируются до его запуска):
1. Сформулировать бизнес-проблему и гипотезу.
2. Определить ключевые и дополнительные метрики.
3. Сформулировать ожидаемый эффект на метрики.
4. Спрогнозировать сроки эксперимента.
5. Установить условия окончания теста, критерий успеха и принципы принятия решения.

**_Конверсия_** - отношение количества пользователей, совершивших целевые действия (клик, покупка и т.п.), к общему количеству пользователей.

Для успешного эксперимента:
* При расчете конверсии необходимо рассчитать **_окно конверсии_** - это можно сделать с помощью исторических данных, взяв оптимальный перцентиль для срока совершения целевого действия.
* Правильно выполнить **_сплитование_** - деление пользователей на группы. Должны сохраняться пропорции в геохарактеристиках, маркетинговых группах или других сегментах.
* Расчитать статистический критерий $(Хи-квадрат)$ при расчете конверсии, 

___
# 3. Корреляция и регрессия <a id="3"></a>
## 3.1 Понятие корреляции <a id="3.1"></a>
**_Ковариация_** - мера _линейной_ зависимости двух случайных величин.

$$ cov(X, Y) = \frac{\sum{(x_i - \bar{x})(y_i - \bar{x})}}{N - 1} $$

Однако только по _абсолютному_ значению ковариации _нельзя судить_ о том, _насколько сильно величины взаимосвязаны_, так как масштаб ковариации зависит от их дисперсий. Значение ковариации можно нормировать, поделив её на произведение среднеквадратических отклонений (квадратных корней из дисперсий) случайных величин. Полученная величина называется **_коэффициентом корреляции Пирсона_**, который всегда находится в интервале от $−1$ до $1$:

$$ r(x, y) = \frac{cov(x, y)}{\sigma_x\sigma_y}$$


* Коэффициент корреляции может принимать значения на промежутке $\left [ -1;1 \right ]$.
* Чем ближе значение коэффициента корреляции к 1 или к -1, тем сильнее взаимосвязь двух переменных.
* Положительное значение коэффициента корреляции говорит о том, что с увеличением значений одной переменной значения второй переменной так же увеличиваются.
* Высокая корреляция не обязательно означает статистически значимую взаимосвязь.
* Коэффициент неустойчив к выбросам.
* Исследуемые переменные должны быть распределены нормально и их взаимосвязь монотонна.
* Результаты только корреляционного исследования не позволяют нам делать вывод о причинно - следственной связи.
* Коэффициент корреляции может быть равен нулю.

**_Влияние третьей переменной_** - ошибка корреляции, при которой удалось получить статистически значимую взаимосвязь двух переменных, но в действительности переменная может содержать в себе другую информацию (третью переменную).

### Подробнее про формулу корреляции

Запишем формулу чуть подробнее и выполним возможные преобразования:

$$ r(x, y) = \frac{\sum{(x_i - \bar{x})(y_i - \bar{y})}}{(N - 1)\sqrt{\sum{\frac{(x_i - \bar{x})^2}{N-1}}}\sqrt{\sum{\frac{(y_i - \bar{y})^2}{N-1}}}} $$

теперь вынесем 1/ (N - 1) из под корней 

$$ r(x, y) = \frac{\sum{(x_i - \bar{x})(y_i - \bar{y})}}{(N - 1)\frac{1}{(N-1)}\sqrt{\sum{(x_i - \bar{x})^2}}\sqrt{\sum{(y_i - \bar{y})^2}}} $$

и сократим (N - 1)

$$ r(x, y) = \frac{\sum{(x_i - \bar{x})(y_i - \bar{y})}}{\sqrt{\sum{(x_i - \bar{x})^2}}\sqrt{\sum{(y_i - \bar{y})^2}}} $$

таким образом, мы сократили N - 1 в знаменателе и получили финальную формулу для коэффициента корреляции, которую часто можно встретить в учебниках:

$$ r(x, y) = \frac{\sum{(x_i - \bar{x})(y_i - \bar{y})}}{\sqrt{\sum{(x_i - \bar{x})^2}\sum{(y_i - \bar{y})^2}}} $$

___
## 3.2 Регрессия с одной независимой переменной <a id="3.2"></a>
**_Линейная регрессия_** — используемая в статистике регрессионная модель зависимости одной (объясняемой, зависимой) переменной $y$ от другой или нескольких других переменных (факторов, регрессоров, независимых переменных) $x$ с _линейной функцией зависимости_.

В общем виде функция линейной регрессии выглядит как:

$$\hat{y}=b_0+b_1x$$

* $b_1$ - **_slope_** задаёт наклон линии регрессии: $b_1 = \frac{sd_y}{sd_x}r_{xy}$
* $b_0$ - **_intercept_** значение пересечения линии с осью $Y$ : $b_0 = \bar{Y} - b_1\bar{X}$

Строят регрессионную прямую методом наименьших квадратов (МНК)

МНК - это способ нахождения оптимальных параметров линейной регресссии $b_0$  и $b_1$, таких, что сумма квадратов ошибок (остатков) была минимальная.

Если коэффициент корреляции между двумя переменными равен нулю, и обе переменные представлены в $z$-значениях, то уравнение регрессии примет вид $y = 0$.

### Условия применения линейной регрессии с одним предиктором
* Линейная взаимосвязь $X$ и $Y$.
* Нормальное распределение остатков.
* **_Гомоскедастичность_** - постоянная изменчивость остатков на всех уровнях независимой переменной.

[Diagnostics for simple linear regression](https://gallery.shinyapps.io/slr_diag/)

___
## 3.3 Гипотеза о значимости взаимосвязи и коэффициент детерминации <a id="3.3"></a>
$R^2$ - коэффициент детерминации, показывает в какой степени дисперсия одной переменной обусловлена влиянием другой переменной. Принимает значения $\left [ -1;1 \right ]$.

$$R^2=r_{xy}^2$$

**_Гипотеза о взаимосвязи двух переменных_** - чем больше коэффициент детерминации, тем большая часть дисперсии зависимой переменной обусловлена взаимосвязью с независимой переменной.

Если коэффициент детерминации равен нулю, то и коэффициент $b_1$ (slope) также равен нулю

$$t=\frac{b_1}{se}$$

при:
* $H_0$: $b_1=0$ (переменные никак не связаны, и линия аппроксимации абсолютно параллельна оси $X$ без наклона)
* $H_1$: $b_1\neq0$
* $df=N-2$

$R^2$ - доля дисперсии зависимой переменной $Y$, объясняемая регрессионной моделью.

$$R^2=1-\frac{SS_{res}}{SS_{total}}$$

где:
* $SS_{res}$ - сумма квадратов остатков (разности между наблюдаемыми значениями и значениями, предсказанными изучаемой регрессионной моделью).
* $SS_{total}$ - сумма квадратов разностей между наблюдаемыми значениями и среднего значения по переменной $Y$.

Если $\frac{SS_{res}}{SS_{total}}\rightarrow 1$, то $R\approx0$.

Если $\frac{SS_{res}}{SS_{total}}\rightarrow 0$, то $R\approx1$.

___
## 3.4 Применение регрессионного анализа и интерпретация результатов <a id="3.4"></a>
Задачи:

1. Построить регрессионную модель, которая максимально удачно объясняет взаимосвязь переменных, рассчитав $b_0$ и $b_1$.
2. Рассчитать коэффициент детерминации, ответив на вопрос "какой процент дисперсии объясняется моделью" - $H_0$: $b_1=0$.
3. Как, основываясь на данных независимой переменной, можно предсказать, чему будет равна зависимая переменная.

Коэффициент $b_i$ при независимой переменной демонстрирует насколько изменяется ожидаемое значение зависимой переменной при единичном изменении независимой переменной при условии, что все остальные независимые переменные _не изменяются._

**_Важное замечание_!** Не всегда линия тренда логически должна идти строго вверх или строго вниз.

___
## 3.5 Регрессионный анализ с несколькими независимыми переменными <a id="3.5"></a>
$$\hat{y}=b_0+b_1x+\ldots+b_nx_n$$

### Требования к данным:
* Линейная зависимость переменных.
* Нормальное распределение остатков.
* **_Гетероскедастичность_**.
* Проверка на мультиколлинеарность.
* Нормальное распределение переменных (желательно).

**_Adjusted_** $R^2$ - скорректированный $R^2$, который рассчитывается при включении в модель дополнительных переменных $(>1)$.

Удобная функция, позволяющий посмотреть на линейную взаимосвязь переменных и их распределение на одном графике.

```python
sns.jointplot(x='hs_grad', y='poverty', data=df, kind='reg')
```

___
# 4. Анализ номинативных данных <a id="4"></a>
## 4.1 Расстояние Пирсона <a id="4.1"></a>
$\chi^2$ используется прежде всего для анализа таблиц сопряженности (вид таблицы, которая учитывает совместное влияние фактора на исход, данные в таблице сопряженности должны быть представлены в виде частоты номинальных данных или интервалами, но не непрерывными количественными величинами).

$$\chi^2=\sum_{i=1}^{n}\frac{(O_i-E_i)^2}{E_i}$$

где $O$ (observed) - наблюдаемая частота, $E$ (expected) - ожидаемая частота.

Для одной номинативной переменной $df=n-1$, где $n$ - количество столбцов таблицы.

### Условия применения критерия
* Желательно, чтобы общее количество наблюдений было более $20$.
* Ожидаемая частота, соответствующая нулевой гипотезе должна быть более $5$, если ожидаемое явление принимает значение менее $5$, то необходимо использовать точный Критерий Фишера.
* Для четырехпольных таблиц $(2х2)$: Если ожидаемое значение принимает значение менее $10$ - а именно $5$ < $x$ < $10$, необходим расчет поправки Йетса таблиц сопряженности.
* Сравниваемые частоты должны быть примерно одного размера.
* Сопоставляемые группы должны быть независимыми (то есть единицы наблюдения в них разные, в отличие от связанных групп, анализирующих изменения «до-после» у одних и тех единиц наблюдений до и после вмешательства. Для таких ситуаций существует отдельный тест МакНемара (McNemar).
* **_Запрещается_**: использовать хи-квадрат для анализа непрерывных абсолютных данных, процентов и долей.
___
## 4.2 Распределение $\chi^2$-квадрат <a id="4.2"></a>
Распределение $\chi^2$-квадрат с $k$ степенями свободы - это распределение суммы квадратов $k$ _независимых стандартных нормальных_ случайных величин.


Чем больше число степеней свободы у распределения хи-квадрат, тем более _симметричным_ становится такое распределение (стремится к нормальному).

<p align="center">
  <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/3/35/Chi-square_pdf.svg/321px-Chi-square_pdf.svg.png">
</p>



### Критические значения $\chi^2$ для $p < \lambda$

<p align="center">
  <img src="https://ucarecdn.com/e7d266af-35e1-44da-acbb-b16932f44842/">
</p>

___
## 4.3 Анализ таблиц сопряженности <a id="4.3"></a>
Для таблиц сопряженности $df=(n-1)*(m-1)$, где $n$ - количество столбцов таблицы, m - число строк таблицы.

Формулу для расчета ожидаемых частот в таблице сопряженности:

$$f_{ij}=\frac{f_i*f_j}{N}$$

* $f_i$ − число наблюдений в $i$−ой строке;
* $f_j$ − число наблюдений в $j$−ом столбце;
* $N$ − общее количество наблюдений в таблице.

### Поправка Йетса

В теориии распределение $\chi^2$ непрерывно, тогда как вычисляемые значения всегда дискретны, в результате $H_0$ может отвергаться слишком часто. Чтобы скорректировать значения $p$-уровня значимости, применяется поправка Йетса на непрерывность.

Обычно применяется, когда некоторые ожидаемые частоты $<10$.

$$\chi_{Yates}^2=\sum^k\frac{(|f_o-f_e|-0,5)^2}{f_e}$$

### Mosaicplot example
Позволяет визуализировать многомерные категориальные данные строгим и информативным способом:

<p align="center">
  <img src="/img/titanic_example.png">
</p>

Выводы, которые можно сделать по графику:

```diff
+ Есть все основания отклонить нулевую гипотезу об отсутствии взаимосвязи пола и вероятности выжить в катастрофе.

+ Значимые отклонения между наблюдаемыми и ожидаемыми результатами получены во всех ячейках что позволяет говорить: мужчины вероятнее погибнут, чем выживут, а женщины — наоборот.

+ На борту Титаника большинство пассажиров - мужчины.

- Значимые отклонения между наблюдаемыми и ожидаемыми результатами получены во всех ячейках, что позволяет говорить: женщины вероятнее погибнут, чем выживут, а мужчины — наоборот.

- Значимые отклонения между наблюдаемыми и ожидаемыми результатами получены только в ячейках: Женщины: Выжили и Мужчины: Не выжили.

- Есть все основания принять нулевую гипотезу о взаимосвязи пола и вероятности выжить в катастрофе.

- Значимые отклонения между наблюдаемыми и ожидаемыми результатами получены только в ячейках: Женщины: Не выжили и Мужчины: Выжили.

- На борту Титаника большинство пассажиров - женщины.
```

___
## 4.4 Точный критерий Фишера <a id="4.4"></a>
Тест $\chi^2$ не является подходящим, когда математические ожидания значений в любой из ячеек таблицы с заданными границами оказывается _ниже_ $10$: вычисленное выборочное распределение испытуемой статистической величины только приблизительно равно теоретическому распределению хи-квадрат, и приближение неадекватно в этих условиях.

**_Тест Фишера_**, как следует из его названия, является точным, и может поэтому использоваться независимо от особенностей выборки. Тест становится трудно вычислимым для больших выборок или хорошо уравновешенных таблиц, но для этих условий хорошо применим критерий $\chi^2$.

|             | Юноши | Девушки | Всего |
|-------------|:-----:|:-------:|:-----:|
| На диете    | 1     | 9       | 10    |
| Не на диете | 11    | 3       | 14    |
| Всего       | 12    | 12      | 24    |

Фишер показал, что вероятность получения любого такого набора величин дается _гипергеометрическим распределением_:

$$P=\frac{(\frac{a+b}{a})(\frac{c+d}{c})}{(\frac{n}{a+c})}=\frac{(a+b)!(c+d)!(a+c)!(b+d)!}{n!a!b!c!d!}$$

Эта формула дает точную вероятность наблюдения любого специфического набора данных, при условии заданных маргинальных итогов, общего итога и нулевой гипотезе об одинаковой предрасположенности к диете независимо от пола (соотношение между людьми, которые на диете, и людьми, не находящимися на диете, для юношей такое же, как для девушек).

___
# 5. Логистическая регрессия и непараметрические методы <a id="5"></a>
## 5.1 Логистическая регрессия <a id="5.1"></a>
### From probability to odds to log of odds (Logit Function)
**_Odds_** - отношение вероятности успеха $(Y=1)$ к вероятности неудачи $(Y=0)$.

$$Logit\ Function=ln(\frac{p}{1-p})$$

После преобразования вероятности в логарифм шансов предсказания регрессионной модели могут быть на промежутке от минус до плюс бесконечности.

<p align="center">
  <img src="/img/logit_function.png">
</p>

### Intercept only model
Свободный член регрессии $intercept$ - это логарифм шанса положительного исхода.

$P < 0.05$ для свободного члена логистической регрессии означает, что:
* Вероятности двух исходов зависимой переменной значимо различаются между собой.
* Распределение исходов в нашей выборке значимо отличается от равномерного.

### Пример модели (Survied ~ Sex + Pclass)

По умолчанию уровни фактора располагаются в _алфавитном порядке_.

<p align="center">
  <img src="/img/log_reg_example.png">
</p>

$$y = 3.3081 - 3.7301 * sex_{male} - 0.8804 * Pclass_{second} - 3.4653 * Pclass_{third} - 0.4204 * sex_{male} * Pclass_{second} +2.1542 * sex_{male} * Pclass_{third}$$

___
## 5.2 Когда нужно использовать непараметрические методы <a id="5.2"></a>
На больших выборках, в силу высокой мощности, даже незначительное отклонение от нормального распределения будет приводить к значимым результатам Shapiro-Wilk test. Если корреляция на графике qqplot _превышает_ $0.95$ - это допустимое отклонение реальных данных от идеальных.

Если $N>30$, то это не освобождает от проверки на _нормальность_ распределения (даже в случае Т-теста).

Применять **_непараметрические тесты_**, если:
* Данные отличаются от нормального распределения.
* Выборки обладают негомогенностью дисперсий.
* Мало данных.

В некоторых задачах непараметрические тесты будут работать _даже лучше_, но это не панацея для всех исследований.

___
## 5.3 U-критерий Манна — Уитни <a id="5.3"></a>
Логика данного критерия заключается в том, что вместо сравнения средних значений в двух выборках критерий сравнивает **_сумму рангов_** (не медианы, как многие думают). Мы сначала упорядочиваем все данные, затем рассчитываем сумму рангов в каждой из групп.

Затем для каждой из выборок рассчитывается показатель:

$U_1=R_1-\frac{n_1*(n_1+1)}{2}$

$U_2=R_2-\frac{n_2*(n_2+1)}{2}$

Где $R_1$, $R_2$ - это сумма рангов в двух группах, а $n_1$, $n_2$ - число наблюдений.

Наименьшее из полученных значений и выступает в качестве статистики теста. Легко показать, что при условии верности нулевой гипотезы распределение этой статистики подчиняется нормальному распределению, где 

$M = \frac{n_{1} *  n_{2}}{2}$ и $σ = \sqrt{\frac{n_1 * n_2 * (n_1 + n_2 + 1)}{12}}$

что и позволяет рассчитать вероятность получить наблюдаемые или еще более выраженные различия суммы рангов.

Применять вместо t - теста: 

```diff
+ Распределения хотя бы в одной из выборок значительно отличается от нормального. 

+ Есть заметные выбросы в данных. 

+ В некоторых задачах мощность теста даже выше, чем t критерия (например, когда обеих выборках наблюдается заметная асимметрия в одинаковом направлении).

- Выборки разного размера, с различным направлением асимметрии.  
```
___
## 5.4 Критерий Краскела-Уоллиса <a id="5.4"></a>
Основная статистика критерия Краскела-Уоллиса - это _дисперсия средних значений рангов_ в сравниваемых группах. При верности $H_0$ распределение этой статистики можно описать при помощи распределения $\chi^2$.

$$H=\frac{12}{N(N+1)} * \sum_{i=1}^{k}\frac{R_{i}^2}{n_{i}}-3*(N+1)$$

$$N=\sum_{i=1}^{k}n_{i}$$

Разумно применять:

```diff
+ В сравниваемых группах распределение выборок значительно отличается от нормального, а также нарушается предположение о гомогенности дисперсий.
```

___
## 5.5 Дорожная карта статистики <a id="5.5"></a>

<p align="center">
  <img src="https://ucarecdn.com/fc1cb807-a1dc-494b-98ea-1f300e53f42f/">
</p>

___

[В начало](#0)
